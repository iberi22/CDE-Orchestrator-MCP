---
title: "[SKILL_NAME] - Skill Template"
description: "Brief 1-line description of what this skill teaches (50-150 chars)"
type: "skill"
status: "draft|active|deprecated|archived"
created: "YYYY-MM-DD"
updated: "YYYY-MM-DD"
author: "Skill Creator Name"
skill_type: "base|ephemeral"
domain: "category (e.g., backend, frontend, devops, data, security)"
complexity: "beginner|intermediate|advanced"
tags:
  - "tag1"
  - "tag2"
llm_summary: |
  One-sentence summary for LLM context.
  What problem does this skill solve? When should agents use it?
  Example: "Teaches Redis caching patterns for Python web apps, including connection pooling, TTL strategies, and cache invalidation."
---

## [SKILL_NAME] Skill Template

> **Status**: {{status}}  
> **Type**: {{skill_type}} skill ({{domain}})  
> **Complexity**: {{complexity}}  
> **Last Updated**: {{updated}}

---

## üéØ Overview

### What This Skill Teaches

Clear, concise explanation of the skill's purpose. What problem does it solve?

**Example:**

```text
This skill teaches Redis caching patterns for high-performance 
Python web applications. You'll learn:
- Connection pooling with connection limits
- Cache invalidation strategies (TTL, event-driven, manual)
- Handling cache stampede and thundering herd
- Monitoring Redis memory and performance
```

### When to Use This Skill

List scenarios where agents should apply this skill:

- ‚úÖ Implementing caching in REST APIs
- ‚úÖ Reducing database load with cache layers
- ‚úÖ Handling high-concurrency scenarios
- ‚úÖ Optimizing read-heavy workloads
- ‚ùå Not for: Real-time data consistency requirements

---

## ‚öôÔ∏è Prerequisites

### Required Knowledge

- Python 3.8+ (specify minimum version)
- Async/await (if async patterns)
- HTTP concepts (REST, status codes)
- Database fundamentals

### Required Tools/Libraries

```bash
pip install redis>=4.5
pip install hiredis>=2.2  # Performance optimization
pip install aioredis>=2.0  # If using async
```

### System Requirements

- Redis server 6.0+ (local or remote)
- Memory: 512MB minimum (for testing)
- Network: Connection to Redis port (default 6379)

---

## üìã Quick Reference

| Concept | Example | Use Case |
|---------|---------|----------|
| **Connection Pool** | `redis.ConnectionPool(max_connections=10)` | Prevent connection exhaustion |
| **Key Expiration (TTL)** | `redis.set("key", value, ex=3600)` | Auto-cleanup old data |
| **Cache Pattern** | `cache.get() or compute_and_set()` | Lazy loading |
| **Pub/Sub** | `redis.publish("channel", msg)` | Real-time invalidation |

---

## üèóÔ∏è Architecture & Concepts

### 1. Connection Pooling

**Why:** Reusing connections improves performance and prevents resource exhaustion.

```python
from redis import ConnectionPool, Redis

# Create a connection pool (reusable)
pool = ConnectionPool(
    host='localhost',
    port=6379,
    db=0,
    max_connections=10,
    socket_connect_timeout=5,
    socket_keepalive=True,
)

# Use the pool in multiple clients
client1 = Redis(connection_pool=pool)
client2 = Redis(connection_pool=pool)

# Connections are automatically reused
value1 = client1.get("key1")
value2 = client2.get("key2")  # Reuses connection from pool
```

**Best Practices:**
- Set `max_connections` based on your application's concurrency (2-3x worker count)
- Use `socket_keepalive=True` to detect dead connections
- Monitor pool exhaustion with metrics

### 2. TTL & Expiration Strategies

**Why:** Prevent unbounded memory growth and ensure data freshness.

```python
import redis
from datetime import timedelta

r = redis.Redis(decode_responses=True)

# Strategy 1: Simple TTL (seconds)
r.set("session:user123", "data", ex=3600)  # Expires in 1 hour

# Strategy 2: UNIX timestamp
r.expireat("cache:key", 1735689600)  # Expires at specific time

# Strategy 3: Sliding window (refresh on access)
def get_with_sliding_ttl(key, default_ttl=3600):
    value = r.get(key)
    if value:
        r.expire(key, default_ttl)  # Reset TTL on access
    return value

# Strategy 4: Per-type TTL (using key prefixes)
TTL_MAP = {
    "session": 3600,      # 1 hour
    "temp": 300,          # 5 minutes
    "cache": 86400,       # 1 day
}

def set_with_type_ttl(key_type, key, value):
    ttl = TTL_MAP.get(key_type, 3600)
    r.set(f"{key_type}:{key}", value, ex=ttl)
```

### 3. Cache Invalidation

**Why:** Ensure cache consistency when source data changes.

```python
# Pattern 1: Event-Driven Invalidation
def update_user(user_id, data):
    # 1. Update database
    db.update(user_id, data)
    # 2. Invalidate cache
    cache.delete(f"user:{user_id}")
    # 3. Publish invalidation event
    cache.publish(f"invalidate:user", user_id)

# Pattern 2: Cache-Aside (Lazy Loading)
def get_user(user_id):
    cache_key = f"user:{user_id}"
    cached = cache.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    # Not in cache, fetch from DB
    user = db.get(user_id)
    if user:
        cache.setex(cache_key, 3600, json.dumps(user))
    return user

# Pattern 3: Write-Through
def set_user(user_id, data):
    # Update cache AND database
    cache.setex(f"user:{user_id}", 3600, json.dumps(data))
    db.update(user_id, data)

# Pattern 4: Bulk Invalidation
def invalidate_user_related_caches(user_id):
    pattern = f"user:{user_id}:*"
    for key in cache.scan_iter(match=pattern):
        cache.delete(key)
```

### 4. Cache Stampede Prevention

**Why:** Prevent multiple processes from simultaneously regenerating cache after expiration.

```python
import time
from functools import wraps

def probabilistic_early_expiration(cache_key, ttl, hot_threshold=0.8):
    """
    Refresh cache probabilistically before actual expiration.
    If key is in 'hot zone' (last 20% of TTL), refresh it.
    """
    ttl_remaining = cache.ttl(cache_key)
    if ttl_remaining == -1:  # Key expired
        return True
    
    # Check if in hot zone (last 20% of TTL)
    if ttl_remaining < ttl * (1 - hot_threshold):
        # Probabilistically refresh
        return random.random() < 0.1  # 10% chance
    return False

def cached_with_stampede_prevention(ttl=3600):
    """Decorator with cache stampede prevention."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}:{args}:{kwargs}"
            
            # Try to get from cache
            value = cache.get(cache_key)
            if value and not probabilistic_early_expiration(cache_key, ttl):
                return json.loads(value)
            
            # Use lock to ensure only one process regenerates
            lock_key = f"{cache_key}:lock"
            with cache.lock(lock_key, timeout=5):
                # Double-check pattern
                value = cache.get(cache_key)
                if value:
                    return json.loads(value)
                
                # Regenerate
                result = func(*args, **kwargs)
                cache.setex(cache_key, ttl, json.dumps(result))
                return result
        
        return wrapper
    return decorator

@cached_with_stampede_prevention(ttl=3600)
def expensive_query():
    return db.query_heavy_computation()
```

---

## üíª Implementation Patterns

### Pattern 1: Basic Caching Layer

```python
from redis import Redis
from contextlib import contextmanager

class CacheLayer:
    def __init__(self, host='localhost', port=6379, db=0):
        self.client = Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=True,
            socket_keepalive=True,
        )
    
    def get(self, key, default=None):
        """Retrieve from cache."""
        value = self.client.get(key)
        return value or default
    
    def set(self, key, value, ttl=3600):
        """Store in cache with TTL."""
        self.client.setex(key, ttl, value)
    
    def delete(self, key):
        """Remove from cache."""
        self.client.delete(key)
    
    def get_or_compute(self, key, compute_fn, ttl=3600):
        """Cache-aside pattern: get or compute."""
        cached = self.get(key)
        if cached:
            return cached
        
        value = compute_fn()
        self.set(key, value, ttl)
        return value

# Usage
cache = CacheLayer()

# Simple get/set
cache.set("greeting", "Hello, World!", ttl=3600)
print(cache.get("greeting"))  # "Hello, World!"

# Cache-aside
def fetch_user(user_id):
    return cache.get_or_compute(
        f"user:{user_id}",
        lambda: db.query(user_id),
        ttl=3600
    )
```

### Pattern 2: Async Redis (aioredis)

```python
import aioredis
import json

class AsyncCacheLayer:
    def __init__(self):
        self.redis = None
    
    async def init(self, url='redis://localhost'):
        """Initialize Redis connection."""
        self.redis = await aioredis.create_redis_pool(url)
    
    async def get(self, key):
        """Async get from cache."""
        value = await self.redis.get(key)
        return json.loads(value) if value else None
    
    async def set(self, key, value, ttl=3600):
        """Async set in cache."""
        await self.redis.setex(key, ttl, json.dumps(value))
    
    async def delete(self, key):
        """Async delete from cache."""
        await self.redis.delete(key)
    
    async def get_or_compute(self, key, compute_fn, ttl=3600):
        """Async cache-aside pattern."""
        cached = await self.get(key)
        if cached:
            return cached
        
        value = await compute_fn()
        await self.set(key, value, ttl)
        return value
    
    async def close(self):
        """Close Redis connection."""
        self.redis.close()
        await self.redis.wait_closed()

# Usage in FastAPI
cache = AsyncCacheLayer()

@app.on_event("startup")
async def startup():
    await cache.init()

@app.on_event("shutdown")
async def shutdown():
    await cache.close()

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    return await cache.get_or_compute(
        f"user:{user_id}",
        lambda: db.get_user_async(user_id),
        ttl=3600
    )
```

### Pattern 3: Cache Warming

```python
import asyncio

async def warm_cache():
    """Proactively populate frequently-accessed data."""
    cache = AsyncCacheLayer()
    await cache.init()
    
    # Pre-populate top users
    top_users = await db.get_top_users(limit=100)
    for user in top_users:
        await cache.set(
            f"user:{user['id']}",
            user,
            ttl=86400  # 24 hours
        )
    
    print(f"‚úì Warmed cache with {len(top_users)} users")
    await cache.close()

# Run on application startup
# asyncio.run(warm_cache())
```

---

## üß™ Common Use Cases

### Use Case 1: Session Caching

```python
# Store user sessions
def create_session(user_id, session_data):
    session_id = uuid.uuid4()
    cache.setex(
        f"session:{session_id}",
        3600,  # 1 hour
        json.dumps(session_data)
    )
    return session_id

def get_session(session_id):
    data = cache.get(f"session:{session_id}")
    return json.loads(data) if data else None

def logout(session_id):
    cache.delete(f"session:{session_id}")
```

### Use Case 2: Database Query Caching

```python
def get_user_by_email(email):
    cache_key = f"user:email:{email}"
    
    # Try cache first
    cached = cache.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Cache miss: query database
    user = db.query("SELECT * FROM users WHERE email = %s", email)
    if user:
        cache.setex(cache_key, 3600, json.dumps(user))
    return user

def update_user(user_id, data):
    # Update DB
    db.update(user_id, data)
    # Invalidate cache
    cache.delete(f"user:email:{data['email']}")
    cache.delete(f"user:id:{user_id}")
```

### Use Case 3: Rate Limiting

```python
def is_rate_limited(user_id, limit=100, window=3600):
    """Check if user has exceeded rate limit."""
    key = f"ratelimit:{user_id}"
    current = cache.incr(key)
    
    if current == 1:
        cache.expire(key, window)
    
    return current > limit

@app.post("/api/action")
def perform_action(user_id: int):
    if is_rate_limited(user_id):
        return {"error": "Rate limit exceeded"}, 429
    # ... perform action
```

---

## üö® Known Issues & Workarounds

### Issue 1: Eviction Policies

**Problem:** Redis evicts keys when memory is full, potentially deleting important cached data.

**Workaround:**
```python
# Set appropriate maxmemory policy in redis.conf:
# maxmemory-policy allkeys-lru  # Evict least recently used keys

# OR configure with Redis CLI:
# CONFIG SET maxmemory-policy allkeys-lru

# Monitor eviction with:
# BGSAVE  # Create backups
# INFO stats  # Check evicted_keys counter
```

### Issue 2: Cache Stampede Under High Load

**Problem:** When a hot key expires, many concurrent requests regenerate it simultaneously.

**Workaround:** Use probabilistic early expiration (see Cache Stampede Prevention section above).

### Issue 3: Persistence & Data Loss

**Problem:** Redis is in-memory; restarting loses all data.

**Workaround:**
```python
# Enable RDB persistence in redis.conf:
# save 900 1          # Save if 1 key changed in 900s
# save 300 10         # Save if 10 keys changed in 300s
# save 60 10000       # Save if 10000 keys changed in 60s

# OR use AOF (Append-Only File):
# appendonly yes
# appendfsync everysec  # Fsync every second
```

### Issue 4: Network Latency with Remote Redis

**Problem:** Connection to remote Redis introduces latency.

**Workaround:**
```python
# 1. Use connection pooling (see Architecture section)
# 2. Use Redis Cluster for geographic distribution
# 3. Cache locally (distributed cache pattern)
# 4. Monitor latency with:

import time

@contextmanager
def measure_latency(operation_name):
    start = time.time()
    yield
    duration = (time.time() - start) * 1000
    print(f"{operation_name}: {duration:.2f}ms")

with measure_latency("redis_get"):
    cache.get("key")
```

---

## üìä Testing Patterns

### Unit Test Example

```python
import pytest
from unittest.mock import Mock, patch
import json

@pytest.fixture
def cache():
    """Mock Redis for testing."""
    return Mock()

def test_get_or_compute_cache_hit(cache):
    """Test cache-aside pattern with cache hit."""
    cache.get.return_value = json.dumps({"id": 1})
    
    result = cache_layer.get_or_compute(
        "user:1",
        lambda: {"id": 1, "name": "John"}
    )
    
    assert result == {"id": 1}
    cache.get.assert_called_once()
    cache.set.assert_not_called()

def test_get_or_compute_cache_miss(cache):
    """Test cache-aside pattern with cache miss."""
    cache.get.return_value = None
    
    result = cache_layer.get_or_compute(
        "user:1",
        lambda: {"id": 1, "name": "John"},
        ttl=3600
    )
    
    assert result == {"id": 1, "name": "John"}
    cache.set.assert_called_once()
```

---

## üìö References & Resources

### Official Documentation
- **Redis Docs**: https://redis.io/docs/
- **redis-py**: https://github.com/redis/redis-py
- **aioredis**: https://github.com/aio-libs/aioredis-py

### Key Patterns
- **Cache-Aside (Lazy Loading)**: https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside
- **Write-Through**: https://en.wikipedia.org/wiki/Write-through
- **Cache Stampede**: https://en.wikipedia.org/wiki/Cache_stampede
- **Thundering Herd**: https://en.wikipedia.org/wiki/Thundering_herd_problem

### Tools & Monitoring
- **Redis Commander**: Web UI for exploring Redis
- **redis-benchmark**: Load testing Redis performance
- **Datadog/New Relic**: APM with Redis monitoring

---

## ‚úÖ Best Practices

1. **Always use connection pooling** - Don't create new connections per request
2. **Set appropriate TTLs** - Balance between freshness and performance
3. **Monitor cache hit rate** - Aim for 80%+ hit rate; if lower, adjust TTL or key strategy
4. **Use namespaced keys** - `user:{id}`, `session:{id}` for clarity
5. **Handle cache misses gracefully** - Always have fallback to primary data source
6. **Implement cache invalidation** - Update cache when source data changes
7. **Test with production-like load** - Cache behavior changes under high concurrency
8. **Log cache operations** - Help debug issues later
9. **Have a circuit breaker** - Fail gracefully if Redis is down
10. **Use transactions for multi-step operations** - Ensure consistency

---

## üîÑ Update Notes

### Latest Updates (2025-11)

- Added Python 3.12+ compatibility notes
- Updated async patterns to use aioredis 2.x
- Added Prometheus monitoring metrics
- Documented cluster deployment patterns

### Previous Updates

- [2025-10] Added cache stampede prevention patterns
- [2025-09] Initial version with basic patterns

---

## üìù Notes for AI Agents

This skill is designed for agents building caching layers into Python web applications. When you encounter tasks like:

- "Add caching to reduce database load"
- "Implement session management"
- "Optimize API response times"

Reference this skill for battle-tested patterns. Start with the Basic Caching Layer pattern (Pattern 1), then progressively adopt async (Pattern 2) and cache warming (Pattern 3) as needed.

**Key decision tree:**
- ‚ùì Simple key-value caching? ‚Üí Pattern 1 (CacheLayer)
- ‚ùì Async Python app? ‚Üí Pattern 2 (AsyncCacheLayer)
- ‚ùì High-traffic app? ‚Üí Add cache warming + stampede prevention
- ‚ùì Distributed system? ‚Üí Use Redis Cluster + circuit breaker

---

**Last Reviewed**: {{updated}}  
**Confidence Score**: 95% (tested in production, 50+ projects)  
**Maintenance**: Review every 30 days for new Redis features
