# src/mcp_tools/template_sync.py
"""
Spec-Kit Template Synchronization and Validation MCP Tools.

Provides manual synchronization and validation of Spec-Kit templates:
- cde_syncTemplates: Download and customize Spec-Kit templates
- cde_validateSpec: Validate generated specs against Spec-Kit standard
"""

import json
import logging
import os
import subprocess
from pathlib import Path

from fastmcp import Context

from ._base import tool_handler
from ._progress_reporter import get_progress_reporter

logger = logging.getLogger(__name__)

# Spec-Kit template URLs
SPEC_KIT_TEMPLATES = {
    "spec.md": "https://raw.githubusercontent.com/github/spec-kit/main/templates/spec-template.md",
    "plan.md": "https://raw.githubusercontent.com/github/spec-kit/main/templates/plan-template.md",
    "tasks.md": "https://raw.githubusercontent.com/github/spec-kit/main/templates/tasks-template.md",
}


@tool_handler
async def cde_syncTemplates(
    ctx: Context,
    project_path: str = ".",
    force: bool = False,
    source: str = "github",
) -> str:
    """
    ðŸ”„ **Sync Spec-Kit Templates** - Download and customize templates from GitHub Spec-Kit.

    Manual template synchronization for power users. Downloads latest templates
    from GitHub Spec-Kit, applies CDE customizations, validates conformity.

    **What it does**:
    1. Downloads templates from GitHub Spec-Kit repository
    2. Backs up existing templates
    3. Applies CDE customizations (llm_summary, MCP Tools, etc.)
    4. Validates conformity (target: 95%+)
    5. Reports changes and recommendations

    **Args**:
        project_path: Path to project (default: current directory)
        force: Overwrite existing templates without backup (default: False)
        source: Source to sync from ('github', 'local') (default: 'github')

    **Returns**:
        JSON with sync results:
        - templates_synced: List of synced templates
        - customizations_applied: CDE extensions added
        - conformity_score: Validation score (0-100)
        - backup_location: Where old templates were saved
        - next_steps: Recommendations

    **Examples**:
        >>> cde_syncTemplates()  # Sync current project
        >>> cde_syncTemplates(force=True)  # Force overwrite
        >>> cde_syncTemplates(project_path="E:\\my-project")  # Specific project

    **Use Cases**:
        1. **Weekly Updates**: Keep templates in sync with upstream Spec-Kit
        2. **New Features**: Get latest template improvements
        3. **Conformity Fix**: Update templates to reach 95%+ conformity
        4. **Team Sync**: Ensure all team members use same template version

    **Integration**:
        - Complements GitHub Actions weekly sync (manual trigger)
        - Uses scripts/customize_templates.py for CDE extensions
        - Uses scripts/validate_spec_kit_conformity.py for validation
    """
    reporter = get_progress_reporter()
    reporter.reset()
    reporter.report_progress("CDE", "syncTemplates", 0.1, "Initializing sync...")

    if not project_path:
        logger.error("Validation failed: project_path cannot be empty.")
        return json.dumps(
            {
                "status": "error",
                "error": "Invalid input",
                "message": "The 'project_path' argument cannot be empty.",
            },
            indent=2,
        )

    if project_path == ".":
        project_path = os.getcwd()

    templates_dir = Path(project_path) / "specs" / "templates"

    # If not forcing, check if templates already exist.
    if not force and any((templates_dir / f).exists() for f in SPEC_KIT_TEMPLATES):
        logger.info("Templates already exist and force=False. Skipping sync.")
        return json.dumps(
            {
                "status": "skipped",
                "message": f"Templates already exist in {templates_dir}. Use force=True to overwrite.",
                "templates_directory": str(templates_dir),
            },
            indent=2,
        )

    templates_dir.mkdir(parents=True, exist_ok=True)

    reporter.report_progress(
        "CDE", "syncTemplates", 0.2, "Downloading templates from GitHub..."
    )

    # Download templates
    downloaded = []
    try:
        import urllib.request

        for template_name, url in SPEC_KIT_TEMPLATES.items():
            template_path = templates_dir / template_name

            # Backup existing
            if template_path.exists() and not force:
                backup_path = templates_dir / f"{template_name}.backup"
                # Remove old backup if exists
                if backup_path.exists():
                    backup_path.unlink()
                    logger.info(f"Removed old backup: {backup_path}")
                template_path.rename(backup_path)
                logger.info(f"Backed up {template_name} to {backup_path}")

            # Download
            urllib.request.urlretrieve(url, str(template_path))
            downloaded.append(template_name)
            logger.info(f"Downloaded {template_name} from {url}")

    except Exception as e:
        logger.error(f"Failed to download templates: {e}")
        return json.dumps(
            {
                "status": "error",
                "error": str(e),
                "message": "Failed to download templates from GitHub",
            },
            indent=2,
        )

    reporter.report_progress(
        "CDE", "syncTemplates", 0.5, "Applying CDE customizations..."
    )

    # Apply customizations
    try:
        scripts_dir = Path(__file__).parent.parent.parent / "scripts"
        customize_script = scripts_dir / "customize_templates.py"

        result = subprocess.run(
            [
                "python",
                str(customize_script),
                "--batch",
                "--input-dir",
                str(templates_dir),
                "--output-dir",
                str(templates_dir),
            ],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            logger.warning(f"Customization warning: {result.stderr}")

        customizations_applied = [
            "llm_summary field added to frontmatter",
            "MCP Tools section added to spec.md",
            "Hexagonal Architecture section added to plan.md",
            "Phase Tracking section added to tasks.md",
        ]

    except Exception as e:
        logger.warning(f"Failed to apply customizations: {e}")
        customizations_applied = []

    reporter.report_progress("CDE", "syncTemplates", 0.8, "Validating conformity...")

    # Validate conformity
    conformity_score = 0
    validation_issues = []
    try:
        validate_script = scripts_dir / "validate_spec_kit_conformity.py"

        subprocess_result = subprocess.run(
            [
                "python",
                str(validate_script),
                "--templates",
                str(templates_dir),
                "--output",
                str(templates_dir / "conformity-report.json"),
            ],
            capture_output=True,
            text=True,
        )

        # Parse conformity report
        report_path = templates_dir / "conformity-report.json"
        if subprocess_result.returncode != 0:
            logger.warning(f"Validation script failed: {subprocess_result.stderr}")
        elif report_path.exists():
            with open(report_path) as f:
                report = json.load(f)
                conformity_score = report.get("average_score", 0)
                validation_issues = report.get("issues", [])

    except Exception as e:
        logger.warning(f"Failed to validate conformity: {e}")

    reporter.report_progress("CDE", "syncTemplates", 1.0, "Sync complete")

    # Generate recommendations
    recommendations = []
    if conformity_score < 95:
        recommendations.append(
            f"âš ï¸ Conformity score ({conformity_score:.1f}%) below target (95%)"
        )
        recommendations.append("   Review validation report for specific issues")
    else:
        recommendations.append(
            f"âœ… Conformity score ({conformity_score:.1f}%) meets target (95%+)"
        )

    if customizations_applied:
        recommendations.append("âœ… CDE customizations applied successfully")
    else:
        recommendations.append("âš ï¸ Customizations may have failed, check manually")

    response = {
        "status": "success",
        "templates_synced": downloaded,
        "customizations_applied": customizations_applied,
        "conformity_score": conformity_score,
        "validation_issues": len(validation_issues),
        "backup_location": str(templates_dir / "*.backup") if not force else None,
        "templates_directory": str(templates_dir),
        "recommendations": recommendations,
        "next_steps": [
            "Review templates in specs/templates/",
            "Check conformity report: specs/templates/conformity-report.json",
            "Use cde_generateSpec to test updated templates",
        ],
    }

    return json.dumps(response, indent=2)


@tool_handler
async def cde_validateSpec(
    ctx: Context,
    spec_directory: str,
    project_path: str = ".",
    strict: bool = False,
) -> str:
    """
    âœ… **Validate Spec** - Validate generated specs against Spec-Kit standard.

    Validates feature specifications for conformity with GitHub Spec-Kit standard.
    Checks YAML frontmatter, section structure, task format, naming conventions.

    **What it does**:
    1. Validates YAML frontmatter (required fields, types, format)
    2. Checks section structure (headings, order, required sections)
    3. Validates task format ([P?] [Story] Description)
    4. Checks naming conventions (lowercase-with-hyphens)
    5. Detects broken internal links
    6. Calculates conformity score (0-100)

    **Args**:
        spec_directory: Directory containing spec.md, plan.md, tasks.md
        project_path: Path to project root (default: current directory)
        strict: Fail if conformity < 95% (default: False)

    **Returns**:
        JSON with validation results:
        - conformity_score: 0-100 score
        - issues: List of validation issues (category, severity, message)
        - passed_checks: List of successful validations
        - recommendations: Suggestions for improvement

    **Examples**:
        >>> cde_validateSpec("specs/my-feature")
        >>> cde_validateSpec("specs/my-feature", strict=True)  # Fail on < 95%

    **Use Cases**:
        1. **Post-Generation**: Validate after cde_generateSpec
        2. **Pre-Commit**: CI/CD validation before merge
        3. **Quality Check**: Ensure specs meet standards
        4. **Template Testing**: Validate template changes

    **Conformity Scoring**:
        - **Errors**: -10 points each (missing required fields, invalid format)
        - **Warnings**: -5 points each (missing optional sections, style issues)
        - **Info**: -1 point each (minor improvements)
        - **Target**: 95%+ for production specs

    **Integration**:
        - Used by GitHub Actions PR validation
        - Complements cde_generateSpec for quality assurance
        - Integrates with CI/CD pipelines
    """
    reporter = get_progress_reporter()
    reporter.reset()
    reporter.report_progress("CDE", "validateSpec", 0.1, "Initializing validation...")

    if not spec_directory:
        logger.error("Validation failed: spec_directory cannot be empty.")
        return json.dumps(
            {
                "status": "error",
                "error": "Invalid input",
                "message": "The 'spec_directory' argument cannot be empty.",
            },
            indent=2,
        )

    if project_path == ".":
        project_path = os.getcwd()

    spec_path = Path(project_path) / spec_directory

    if not spec_path.exists():
        logger.error(f"Validation failed: Spec directory not found at {spec_path}")
        return json.dumps(
            {
                "status": "error",
                "error": "Directory not found",
                "message": f"The specified directory does not exist: {spec_path}",
            },
            indent=2,
        )

    reporter.report_progress("CDE", "validateSpec", 0.5, "Running validation checks...")

    # Run validation script
    try:
        scripts_dir = Path(__file__).parent.parent.parent / "scripts"
        validate_script = scripts_dir / "validate_spec_kit_conformity.py"

        output_path = spec_path / "validation-report.json"

        cmd_args = [
            "python",
            str(validate_script),
            "--spec",
            str(spec_path),
            "--min-score",
            "95" if strict else "0",
            "--output",
            str(output_path),
        ]

        if strict:
            cmd_args.append("--fail-on-low-score")

        result = subprocess.run(
            cmd_args,
            capture_output=True,
            text=True,
        )

        # Log script output for debugging, but don't fail immediately
        if result.returncode != 0:
            logger.warning(
                f"Validation script exited with code {result.returncode} "
                f"(expected in strict mode). Stderr: {result.stderr}"
            )

        # Prioritize parsing the report, as it may exist even if the script "failed"
        if not output_path.exists():
            logger.error(f"Validation report not found at {output_path}")
            return json.dumps(
                {
                    "status": "error",
                    "error": "Validation report was not generated.",
                    "details": "The validation script failed to produce an output file.",
                    "stderr": result.stderr,
                    "stdout": result.stdout,
                },
                indent=2,
            )

        # Parse validation report
        with open(output_path) as f:
            report = json.load(f)

        reporter.report_progress("CDE", "validateSpec", 1.0, "Validation complete")

        # Extract results
        conformity_score = float(report.get("average_score", 0))
        issues = []
        passed_checks = []
        files_analyzed = []

        # Extract issues from all templates
        results = report.get("results", {})
        if isinstance(results, dict):
            files_analyzed = list(results.keys())
            for template_name, template_data in results.items():
                template_issues = template_data.get("issues", [])
                issues.extend(template_issues)
                template_passed = template_data.get("passed_checks", [])
                passed_checks.extend(template_passed)

        # New: Categorize issues for test compatibility
        categories_map = {
            "file": "required_files",
            "frontmatter": "yaml_frontmatter",
            "naming": "naming_conventions",
            "link": "content_quality",
            "task_format": "content_quality",
            "section": "structure",
        }
        categorized_issues = {
            "required_files": [],
            "yaml_frontmatter": [],
            "naming_conventions": [],
            "content_quality": [],
            "structure": [],
        }

        for issue in issues:
            script_category = issue.get("category", "unknown")
            test_category = categories_map.get(script_category)
            if test_category and test_category in categorized_issues:
                categorized_issues[test_category].append(issue)

        # Generate recommendations
        recommendations = []
        if conformity_score >= 95:
            recommendations.append(f"âœ… Excellent conformity ({conformity_score:.1f}%)")
        else:
            recommendations.append(
                f"âŒ Low conformity ({conformity_score:.1f}%), review issues below"
            )

        errors = [i for i in issues if i.get("severity") == "error"]
        if errors:
            recommendations.append(f"ðŸ”´ Fix {len(errors)} errors (high priority)")

        result_data = {
            "status": "success" if conformity_score >= 95 or not strict else "warning",
            "conformity_score": conformity_score,
            "overall_score": conformity_score,  # For test compatibility
            "categories": categorized_issues,
            "files_analyzed": files_analyzed,
            "total_issues": len(issues),
            "issues": issues,
            "passed_checks": passed_checks,
            "recommendations": recommendations,
            "validation_report": str(output_path),
        }

        return json.dumps(result_data, indent=2)

    except Exception as e:
        logger.error(f"Validation failed: {e}")
        return json.dumps(
            {
                "status": "error",
                "error": str(e),
                "message": "Validation failed. Check logs for details.",
            },
            indent=2,
        )
